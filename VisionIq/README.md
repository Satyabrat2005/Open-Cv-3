# üß† VisionIQ
### Ask Your Video Anything

> VisionIQ is a local-first video intelligence engine that converts raw video into searchable, explainable, evidence-grounded knowledge.

VisionIQ does not guess.  
VisionIQ does not hallucinate.  
VisionIQ **observes, remembers, and retrieves truth from video**.

This is not object detection.  
This is not captioning.  
This is **video intelligence infrastructure**.

---

## üöÄ What Is VisionIQ?

Video is everywhere ‚Äî security cameras, meetings, factories, hospitals, research labs.  
Yet video remains the **least intelligent and least usable form of data**.

VisionIQ fixes that.

It transforms raw video into a **persistent semantic memory** that you can query using natural language.

Upload a video.  
VisionIQ watches it frame by frame.  
Ask a question.  
VisionIQ retrieves the exact visual evidence that answers it.

---

## üß© What Can You Ask?

- Where is the bottle in the video?
- When does the laptop first appear?
- What happens before the bottle becomes visible?
- Show scenes with both a backpack and a laptop.
- Which objects appear throughout the video?

VisionIQ answers using **what the video actually shows** ‚Äî not assumptions, not guesses.

---

## üéØ Why VisionIQ Exists

Most AI systems treat video as:
- static frames
- autogenerated captions
- black-box summaries

VisionIQ treats video as **experience over time**.

It combines:
- Vision
- Semantics
- Memory
- Retrieval

The result is **searchable, explainable, and auditable video understanding**.

---

## üß† Core Capabilities (V1)

### Video Understanding
- Frame-by-frame video processing
- Temporal awareness (before / after / during)
- Object-level perception
- Optimized for DeepSeek-R1 integration

### Semantic Search
- Text-to-frame similarity search
- Robust to partial visibility
- Ranked, deterministic retrieval

### Intelligent Memory
- Vector-based storage using FAISS
- Persistent frame-level indexing
- Reusable intelligence (process once, query forever)

### Privacy First
- Fully local execution
- No cloud dependency
- Safe for sensitive and private videos

---

## üß† Advanced Capabilities (Planned)

- LLM-based reasoning over retrieved evidence
- Natural language answers with traceable explanations
- Multi-object logical queries (AND / OR)
- Cross-video semantic search
- Timeline-aware reasoning

---

## üèóÔ∏è System Architecture

Video
‚Üì
Frame Extraction (OpenCV)
‚Üì
Object Detection (YOLOv8)
‚Üì
Semantic Embeddings (CLIP)
‚Üì
Vector Memory (FAISS)
‚Üì
Query Engine
‚Üì
Ranked Visual Evidence
‚Üì
(LLM Reasoning ‚Äì Optional Layer)

The LLM never sees raw video.  
It reasons only over **filtered, retrieved evidence**.

---

## üß∞ Technology Stack

| Layer | Technology |
|-----|-----------|
| Video Processing | OpenCV |
| Object Detection | YOLOv8 |
| Embeddings | CLIP |
| Vector Store | FAISS |
| Query Engine | Python |
| Reasoning (Optional) | DeepSeek-R1 |
| Deployment | Local / On-Prem / SaaS-Ready |

---

## üìÅ Project Structure

visioniq/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ video_processor.py     # Frame extraction
‚îÇ   ‚îú‚îÄ‚îÄ object_detector.py     # YOLO detection
‚îÇ   ‚îú‚îÄ‚îÄ embedder.py            # Embedding generation
‚îÇ   ‚îú‚îÄ‚îÄ database.py            # FAISS vector memory
‚îÇ   ‚îú‚îÄ‚îÄ query_engine.py        # Retrieval logic
‚îÇ   ‚îú‚îÄ‚îÄ llm_engine.py          # (Optional) LLM reasoning
‚îÇ   ‚îî‚îÄ‚îÄ main.py
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ videos/
‚îÇ   ‚îú‚îÄ‚îÄ frames/
‚îÇ   ‚îî‚îÄ‚îÄ embeddings/
‚îÇ
‚îú‚îÄ‚îÄ vision-iq-env/             # Virtual environment
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md

---

## üíª Hardware Requirements

Recommended minimum:

- GPU: RTX 3060 / 4060 (8GB VRAM)
- RAM: 16 GB
- Storage: 20 GB+
- OS: Windows or Linux

Scales cleanly with better hardware.

---

## ‚öôÔ∏è Installation

Create virtual environment:

py -3.10 -m venv vision-iq-env

Activate environment (Windows):

.\vision-iq-env\Scripts\Activate.ps1

Install dependencies:

pip install -r requirements.txt

---

## üîç Basic Query Engine (V1)

VisionIQ already supports **semantic video querying without any LLM**.

This baseline engine:
- embeds video frames
- stores them in vector memory
- retrieves relevant frames for text queries

Pure vision. Pure semantics. Zero hallucination.

---

## ‚ñ∂Ô∏è Run the Query Engine

cd src
python test_query_engine.py

### Example Query

>> where is bottle

### Output (LLM-Free, Evidence-Only)

üí° VisionIQ Query (type 'exit' to quit)

ANSWER
--------------------------------------------------
I found 5 relevant scene(s) for your query:

1. Frame frame_00004.jpg (similarity 0.27)
2. Frame frame_00001.jpg (similarity 0.26)
3. Frame frame_00005.jpg (similarity 0.26)
4. Frame frame_00000.jpg (similarity 0.26)
5. Frame frame_00007.jpg (similarity 0.25)

üì∑ MATCHED FRAMES
--------------------------------------------------
Rank 1 | Score: 0.270 | Frame: frame_00004.jpg
Rank 2 | Score: 0.262 | Frame: frame_00001.jpg
Rank 3 | Score: 0.259 | Frame: frame_00005.jpg
Rank 4 | Score: 0.256 | Frame: frame_00000.jpg
Rank 5 | Score: 0.248 | Frame: frame_00007.jpg

---

## üß† VisionIQ Philosophy

AI should not just generate text.  
It should **understand reality**.

VisionIQ is built on the principle that:
- intelligence must be grounded
- memory must be persistent
- reasoning must be auditable

---

## üè¢ About NeuroTitan

VisionIQ is developed under **NeuroTitan AI Labs**, a research-driven initiative focused on:

- Applied AI systems
- Cognitive infrastructure
- Local-first intelligence
- AI‚Äìsemiconductor co-design

---

## ‚≠ê Final Note

VisionIQ is not a demo.  
It is not a chatbot.  
It is not a toy.

It is **infrastructure for intelligent video systems**.

If you believe video should be **searchable, explainable, and trustworthy** ‚Äî  
you‚Äôre in the right place.
