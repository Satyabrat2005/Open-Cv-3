import cv2
import mediapipe as mp
import pyautogui
import numpy as np
import time
import math

# Mediapipe hands
mp_hands = mp.solutions.hands # type: ignore
hands = mp_hands.Hands(max_num_hands=1)
mp_draw = mp.solutions.drawing_utils # type: ignore

# Screen size
screen_w, screen_h = pyautogui.size()

# Cursor smoothing
prev_x, prev_y = 0, 0
smooth_factor = 0.15  # exponential smoothing factor
prediction_factor = 0.3  # how much to predict ahead

# Click cooldown
click_cooldown = 0.5
last_click_time = 0

# Drag state
dragging = False

# Webcam
cap = cv2.VideoCapture(0)
wCam, hCam = 640, 480
cap.set(3, wCam)
cap.set(4, hCam)
frame_margin = 100

def fingers_up(lm_list):
    fingers = []
    fingers.append(lm_list[4][0] > lm_list[3][0])          # Thumb
    fingers.append(lm_list[8][1] < lm_list[6][1])          # Index
    fingers.append(lm_list[12][1] < lm_list[10][1])        # Middle
    fingers.append(lm_list[16][1] < lm_list[14][1])        # Ring
    fingers.append(lm_list[20][1] < lm_list[18][1])        # Pinky
    return fingers

def distance(p1, p2):
    return math.hypot(p1[0]-p2[0], p1[1]-p2[1])

# Previous finger position for prediction
prev_index_x, prev_index_y = 0, 0
prev_time = time.time()

while True:
    success, frame = cap.read()
    frame = cv2.flip(frame, 1)
    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    result = hands.process(rgb_frame)
    
    if result.multi_hand_landmarks:
        for handLms in result.multi_hand_landmarks:
            mp_draw.draw_landmarks(frame, handLms, mp_hands.HAND_CONNECTIONS)
            lm_list = [(int(lm.x*wCam), int(lm.y*hCam)) for lm in handLms.landmark]

            index_x, index_y = lm_list[8]
            thumb_x, thumb_y = lm_list[4]

            # Predict finger movement based on velocity
            current_time = time.time()
            dt = current_time - prev_time
            if dt == 0:
                dt = 0.001
            vel_x = (index_x - prev_index_x) / dt
            vel_y = (index_y - prev_index_y) / dt
            pred_index_x = index_x + vel_x * prediction_factor
            pred_index_y = index_y + vel_y * prediction_factor
            prev_index_x, prev_index_y = index_x, index_y
            prev_time = current_time

            # Map predicted coordinates to screen
            target_x = np.interp(pred_index_x, [frame_margin, wCam-frame_margin], [0, screen_w])
            target_y = np.interp(pred_index_y, [frame_margin, hCam-frame_margin], [0, screen_h])

            # Ultra-smooth exponential smoothing
            prev_x += (target_x - prev_x) * smooth_factor
            prev_y += (target_y - prev_y) * smooth_factor
            pyautogui.moveTo(prev_x, prev_y)

            # Draw pointer & cursor
            cv2.circle(frame, (index_x, index_y), 12, (255, 0, 0), cv2.FILLED)
            cv2.circle(frame, (int(prev_x * wCam / screen_w), int(prev_y * hCam / screen_h)), 15, (0, 255, 0), 2)

            # Fingers state
            fingers = fingers_up(lm_list)

            # Selection click: index + middle fingers
            if fingers[1] and fingers[2] and not dragging:
                if time.time() - last_click_time > click_cooldown:
                    pyautogui.click()
                    last_click_time = time.time()
                    cv2.circle(frame, (index_x, index_y), 20, (0, 255, 0), cv2.FILLED)

            # Drag & Drop: thumb + index pinch
            if distance((index_x, index_y), (thumb_x, thumb_y)) < 40:
                if not dragging:
                    pyautogui.mouseDown()
                    dragging = True
                cv2.circle(frame, (index_x, index_y), 20, (0, 0, 255), cv2.FILLED)
            else:
                if dragging:
                    pyautogui.mouseUp()
                    dragging = False

    cv2.imshow("Ultra Smooth Predicted Mouse", frame)
    if cv2.waitKey(1) & 0xFF == 27:
        break

cap.release()
cv2.destroyAllWindows()

